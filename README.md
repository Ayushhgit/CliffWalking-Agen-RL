# Reinforcement Learning on CliffWalking

I coded out SARSA and Q-Learning on the Cliff Walking Problem with tabular action-value function.

##### Cliff Walking Results

![Cliff Walking](https://user-images.githubusercontent.com/53657825/178178405-fe853845-cd5d-4c8f-a679-1d2592ae18b5.gif)


This repository implements **Q-Learning** and **SARSA** algorithms on the **CliffWalking** environment using [Gymnasium](https://gymnasium.farama.org/).
The project demonstrates how **on-policy (SARSA)** and **off-policy (Q-Learning)** temporal-difference learning differ in their behavior and learned policies.

---

## ğŸ“Œ Problem Definition: CliffWalking Environment

* Gridworld: **4 Ã— 12 grid**
* Start state **(S)** = bottom-left corner
* Goal state **(G)** = bottom-right corner
* Cliff = all squares between **S** and **G** along the bottom row

### Rules:

* Reward = **âˆ’1** for each step
* Reward = **âˆ’100** if the agent falls into the cliff (agent resets to start)
* Goal: Reach **G** with maximum cumulative reward while avoiding the cliff

<p align="center">
  <img src="https://gymnasium.farama.org/_images/cliffwalking.png" width="500"/>
</p>

---

## ğŸ“– Theoretical Background

### ğŸ”¹ Markov Decision Process (MDP)

The environment is modeled as an MDP defined by:
[
\langle S, A, P, R, \gamma \rangle
]

* **S** â†’ finite set of states (48 states for CliffWalking)
* **A** â†’ set of actions (Up, Down, Left, Right)
* **P** â†’ transition probabilities
* **R** â†’ reward function
* **Î³ (gamma)** â†’ discount factor for future rewards

---

### ğŸ”¹ Q-Function

The **action-value function** (Q-function) is defined as:
[
Q^\pi(s,a) = \mathbb{E}*\pi \Big[ \sum*{t=0}^\infty \gamma^t r_{t+1} ; \Big| ; s_0 = s, a_0 = a \Big]
]

It represents the expected return when taking action `a` in state `s` under policy `Ï€`.

---

### ğŸ”¹ Temporal Difference (TD) Learning

TD learning updates Q-values using **bootstrapping**:
[
Q(s,a) \leftarrow Q(s,a) + \alpha \big[ \text{Target} - Q(s,a) \big]
]

Where:

* **Î±** = learning rate
* **Target** depends on the algorithm (SARSA or Q-Learning).

---

### ğŸ”¹ SARSA (On-Policy TD Control)

* Uses the **actual next action chosen** by the current Îµ-greedy policy.
* Update rule:
  [
  Q(s,a) \leftarrow Q(s,a) + \alpha \Big[ r + \gamma Q(s',a') - Q(s,a) \Big]
  ]
* **On-policy** â†’ learns values consistent with the current exploration strategy.
* Learns a **safer path**, avoiding the cliff more.

---

### ğŸ”¹ Q-Learning (Off-Policy TD Control)

* Uses the **best possible next action** (`max_a Q[sâ€™,a]`) for updates.
* Update rule:
  [
  Q(s,a) \leftarrow Q(s,a) + \alpha \Big[ r + \gamma \max_{a'} Q(s',a') - Q(s,a) \Big]
  ]
* **Off-policy** â†’ learns the optimal greedy policy regardless of exploration.
* Learns a **risky shortest path**, hugging the cliff.

---

## âš™ï¸ Implementation

* **Language**: Python 3
* **Libraries**:

  * `gymnasium` â†’ environment
  * `numpy` â†’ Q-table operations
  * `opencv-python` â†’ custom visualization
  * `pickle` â†’ save/load learned Q-tables

### ğŸ“ Files

* `q_learning.py` â†’ Q-Learning training
* `sarsa.py` â†’ SARSA training
* `visualize.py` â†’ OpenCV-based visualization of episodes
* `q_learning_q_table.pkl` / `sarsa_q_table.pkl` â†’ saved models

---

## ğŸ“Š Experimental Results

* **Q-Learning**:

  * Learns a **shortest risky path** near the cliff.
  * Optimizes for maximum return but risks falling.

* **SARSA**:

  * Learns a **longer but safer path** avoiding the cliff.
  * Reflects on-policy exploration behavior.

<p align="center">
  <b>Comparison:</b><br>
  Q-Learning â†’ Risky Optimal Policy âš¡ <br>
  SARSA â†’ Safer Conservative Policy ğŸ›¡ï¸
</p>

---

## â–¶ï¸ How to Run

### 1ï¸âƒ£ Install dependencies

```bash
pip install gymnasium opencv-python numpy
```

### 2ï¸âƒ£ Train Q-Learning

```bash
python q_learning.py
```

### 3ï¸âƒ£ Train SARSA

```bash
python sarsa.py
```

### 4ï¸âƒ£ Visualize Agent

```bash
python visualize.py
```

---

## ğŸ”® Future Extensions

* Compare convergence speed between Q-Learning and SARSA.
* Add **Expected SARSA** implementation.
* Extend to **Deep Q-Learning (DQN)**.
* Implement reward shaping for safer exploration.

---

## ğŸ“š References

* [Gymnasium CliffWalking Docs](https://gymnasium.farama.org/environments/toy_text/cliff_walking/)

---

